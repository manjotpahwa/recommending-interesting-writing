{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded.\n",
      "Data loaded.\n",
      "Model Loaded.\n",
      "Final:  1597\n",
      "Filtered, Mapped Data saved to \\users\\rohan\\news-classification\\Data\\corona-feeds\\mapped-data directory\n",
      "-------------------\n",
      "Article-Word Matrix Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnp.save(\"word_emb.npy\", word_emb)\\nprint(\"Word Embeddings Saved\")\\n\\nnp.save(\"word_bias.npy\", word_bias)\\nprint(\"Word Biases Saved\")\\n\\nwith open(\"/users/rohan/news-classification/Data/feed_ranks/data/mapped-data/mapped_dataset.json\",\\'r\\') as file:\\n    real_data = json.load(file)\\n    \\nwith open(\\'/users/rohan/news-classification/data/dictionaries/reversed_word_ids.json\\', \"r\") as file:\\n    id_to_word = json.load(file)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "sys.path.append(\"../../default-approach\")\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import scipy\n",
    "import boto3\n",
    "\n",
    "dict_dir = Path(\"../../../data/dictionaries\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)\n",
    "print(\"Dictionaries loaded.\")\n",
    "\n",
    "data_path = Path('../../../data/corona-feeds/scrape_data/corona-rss-articles-info.json')\n",
    "dataset = Articles(data_path)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "abs_model_path = Path(\"../../../data/gridsearch_results/2020-05-25/optimizer_type=RMS_use_all_words=False_emb_size=100_learning_rate=0.0001_word_embedding_type=mean/model/mean-inner-product-model.pt\")\n",
    "kwargs = dict(n_publications=len(final_publication_ids),\n",
    "              n_articles=len(final_url_ids),\n",
    "              n_attributes=len(final_word_ids),\n",
    "              emb_size=100,\n",
    "              sparse=False,\n",
    "              use_article_emb=False,\n",
    "              mode='mean')\n",
    "model = InnerProduct(**kwargs)\n",
    "model.load_state_dict(torch.load(abs_model_path))\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "dataset.tokenize()\n",
    "proper_data = dataset.map_items(final_word_ids,\n",
    "                    final_url_ids,\n",
    "                    final_publication_ids,\n",
    "                    filter=True,\n",
    "                    min_length=200)\n",
    "\n",
    "data_path = Path(\"/users/rohan/news-classification/Data/corona-feeds\")\n",
    "if not data_path.is_dir():\n",
    "    data_path.mkdir()\n",
    "mapped_data_path = data_path / \"mapped-data\"\n",
    "if not mapped_data_path.is_dir():\n",
    "    mapped_data_path.mkdir()\n",
    "train_mapped_path = mapped_data_path / \"mapped_dataset.json\"\n",
    "with open(train_mapped_path, \"w\") as file:\n",
    "    json.dump(proper_data, file)\n",
    "raw_data = Articles(train_mapped_path)\n",
    "print(\"Final: \", len(raw_data))\n",
    "print(f\"Filtered, Mapped Data saved to {mapped_data_path} directory\")\n",
    "print(\"-------------------\")\n",
    "\n",
    "word_articles = scipy.sparse.csr_matrix((len(raw_data), len(final_word_ids)), dtype=np.float32).toarray()\n",
    "\n",
    "for idx, item in enumerate(raw_data.examples):\n",
    "    item['text'] = list(set(item['text']))\n",
    "    for entry in item['text']:\n",
    "        word_articles[idx][entry] = 1\n",
    "\n",
    "publication_emb = model.publication_embeddings.weight.data[0].cpu().numpy()\n",
    "publication_bias = model.publication_bias.weight.data[0].cpu().numpy()\n",
    "word_emb = model.attribute_emb_sum.weight.data.cpu().numpy()\n",
    "word_bias = model.attribute_bias_sum.weight.data.cpu().numpy()\n",
    "scipy.sparse.save_npz(\"/users/rohan/news-classification/Data/demo-data/sets/csr_articles_corona.npz\", csr_matrix(word_articles))\n",
    "print(\"Article-Word Matrix Saved\")\n",
    "\n",
    "'''\n",
    "np.save(\"word_emb.npy\", word_emb)\n",
    "print(\"Word Embeddings Saved\")\n",
    "\n",
    "np.save(\"word_bias.npy\", word_bias)\n",
    "print(\"Word Biases Saved\")\n",
    "\n",
    "with open(\"/users/rohan/news-classification/Data/feed_ranks/data/mapped-data/mapped_dataset.json\",'r') as file:\n",
    "    real_data = json.load(file)\n",
    "    \n",
    "with open('/users/rohan/news-classification/data/dictionaries/reversed_word_ids.json', \"r\") as file:\n",
    "    id_to_word = json.load(file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully!\n",
      "0.7484641075134277\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import json\n",
    "import scipy\n",
    "\n",
    "publication_emb = np.asarray([1.0440499, 1.0030843, 1.0340449, 0.992087, 1.0509816, 1.0315005, -1.0493797, -1.0198538, 0.9712321, -1.026394, \n",
    "            -0.9687971, 1.0592866, -1.0200703, -1.0423145, 0.9929519, 1.0220934, 1.021279, -1.0265925, 0.9601833, 0.9763889, \n",
    "            1.0109168, -0.9728226, 0.97199583, -1.0237931, -0.9996001, 0.9932069, 0.97966635, -0.98893607, -0.9876815, -0.98812914, \n",
    "            -0.9625895, 0.99879754, 0.9876508, -0.9581506, -0.95436096, -0.9601925, -1.0134513, -0.98763955, 0.98665, -1.0140482, \n",
    "            1.004904, 0.9894275, -1.0044671, -0.9839679, -0.97082543, -0.9798079, 0.9926766, -0.97317344, 0.9797, -0.97642475, \n",
    "            -0.99420726, -0.9972062, -1.0104703, 1.0575777, 0.9957696, -1.0413874, -1.0056863, -1.0151271, -0.99969465, 0.97463423, \n",
    "            -0.98398715, -1.0211866, -1.0128828, -1.0024365, -0.9800189, 1.0457181, 1.0155835, -1.036794, -1.013707, -1.0498024, \n",
    "            -1.0252678, -1.0388161, -0.97501564, 0.97687274, 0.97906756, 1.0536852, 1.0590494, -0.96917725, 1.0247189, -0.9818878, \n",
    "            -1.0417286, -1.0204054, -1.0285249, -1.0329671, 0.9705739, 0.96375024, 0.9891868, 0.9892464, 1.039075, 1.0042666,\n",
    "            0.9786834, 1.0199072, 0.98080486, 0.9698635, -0.99322844, -0.95841753, -0.99150276, 0.97394156, 0.9976019, -1.0375009], dtype=np.float32)\n",
    "\n",
    "publication_bias = 0.99557\n",
    "word_articles = scipy.sparse.load_npz('/users/rohan/news-classification/Data/demo-data/sets/csr_articles_corona.npz')\n",
    "word_emb = np.load('/users/rohan/news-classification/Data/demo-data/word_emb.npy')\n",
    "word_bias = np.load('/users/rohan/news-classification/Data/demo-data/word_bias.npy')\n",
    "\n",
    "with open(\"/users/rohan/news-classification/Data/corona-feeds/mapped-data/mapped_dataset.json\",'r') as file:\n",
    "    real_data = json.load(file)\n",
    "    \n",
    "with open('/users/rohan/news-classification/data/dictionaries/reversed_word_ids.json', \"r\") as file:\n",
    "    id_to_word = json.load(file)\n",
    "print(\"Data Loaded Successfully!\")\n",
    "\n",
    "time1 = time.time()\n",
    "article_embeddings = word_articles.dot(word_emb)\n",
    "\n",
    "emb_times_publication = np.dot(article_embeddings, publication_emb.reshape(100,1))\n",
    "\n",
    "article_bias = word_articles.dot(word_bias)\n",
    "\n",
    "product_with_bias = emb_times_publication + article_bias\n",
    "\n",
    "word_counts = word_articles.sum(axis=1).reshape(word_articles.shape[0], 1)\n",
    "\n",
    "final_logits = np.divide(product_with_bias, word_counts) + float(publication_bias)\n",
    "\n",
    "indices = final_logits.argsort(axis=0)[-75:].reshape(75)\n",
    "\n",
    "word_logits = np.dot(word_emb, publication_emb.reshape(100,1)) + word_bias\n",
    "\n",
    "top_articles = word_articles[indices.tolist()[0]]\n",
    "\n",
    "broadcasted_words_per_article = top_articles.toarray() * word_logits.T\n",
    "\n",
    "sorted_word_indices = broadcasted_words_per_article.argsort(axis=1)\n",
    "\n",
    "return_articles = []\n",
    "\n",
    "i=0\n",
    "for idx in indices.tolist()[0]:\n",
    "    current_article = raw_data[int(idx)]\n",
    "    current_article['logit'] = float(final_logits[int(idx)])\n",
    "    current_sorted_words = sorted_word_indices[i]\n",
    "    top_words = []\n",
    "    least_words = []\n",
    "    for top_word in current_sorted_words[-10:]:\n",
    "        word = id_to_word[str(top_word)]\n",
    "        top_words.append(word)\n",
    "    for least_word in current_sorted_words[:10]:\n",
    "        word = id_to_word[str(least_word)]\n",
    "        least_words.append(word)\n",
    "    current_article['top_words'] = top_words\n",
    "    current_article['least_words'] = least_words\n",
    "    return_articles.append(current_article)\n",
    "    i+=1\n",
    "    \n",
    "time2 = time.time()\n",
    "\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Things Considered : NPR\n",
      "The New Criterion Latest Blog\n",
      "The New York Review of Books\n",
      "n+1\n",
      "n+1\n",
      "n+1\n",
      "All Things Considered : NPR\n",
      "The New Republic\n",
      "Bitcoin Magazine\n",
      "First Things RSS Feed - Web Exclusives | Daily Writings From Our Top Writers | First Things\n",
      "n+1\n",
      "n+1\n",
      "n+1\n",
      "The Walrus\n",
      "n+1\n",
      "n+1\n",
      "The New Criterion Latest Blog\n",
      "Aeon\n",
      "The New York Review of Books\n",
      "The New Criterion Latest Blog\n",
      "Commonweal Magazine\n",
      "City Journal\n",
      "Sydney Morning Herald - Latest News\n",
      "NYT > Top Stories\n",
      "Economist Books and arts\n",
      "The New York Review of Books\n",
      "WSJ Opinion\n",
      "City Journal\n",
      "News : NPR\n",
      "n+1\n",
      "American Banker: Feed\n",
      "Economist Business\n",
      "The American Spectator\n",
      "n+1\n",
      "Morning Edition : NPR\n",
      "The New Criterion Latest Blog\n",
      "n+1\n",
      "n+1\n",
      "Everything\n",
      "Project Syndicate\n",
      "n+1\n",
      "The Globalist\n",
      "Economist Special report\n",
      "Project Syndicate\n",
      "Dissent Magazine\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7f17ef7b95ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mgrand_html\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"</td>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publication'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publication'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m35\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mgrand_html\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"<td class=\\\"publication\\\">{article['publication']}</td>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "ordered_return_articles = return_articles[::-1]\n",
    "grand_html = []\n",
    "for idx, article in enumerate(ordered_return_articles):\n",
    "    grand_html.append(\"<tr>\")\n",
    "    grand_html.append(f\"<td class=\\\"title mdl-data-table__cell--non-numeric\\\"><a class=\\\"article_link\\\" href=\\\"{article['link']}\\\">{article['title']}</a>\")\n",
    "    grand_html.append(\"<br></br>\")\n",
    "    grand_html.append(\"<br></br>\")\n",
    "    top_word_list = \"\"\n",
    "    for item in article['top_words']:\n",
    "        if len(item) > 2 and not RepresentsInt(item):\n",
    "            top_word_list += item\n",
    "            top_word_list += \", \"\n",
    "    least_word_list = \"\"\n",
    "    for item in article['least_words']:\n",
    "        if len(item) > 2:\n",
    "            least_word_list += item\n",
    "            least_word_list += \", \"\n",
    "    grand_html.append(f\"<p class=\\\"top_words\\\">{top_word_list[:-2]}</p>\")\n",
    "    grand_html.append(f\"<p class=\\\"least_words\\\">{least_word_list[:-2]}</p>\")\n",
    "    grand_html.append(\"</td>\")\n",
    "    print(article['publication'])\n",
    "    if not article['Publication'] or len(article['publication']) > 35:\n",
    "        continue\n",
    "    grand_html.append(f\"<td class=\\\"publication\\\">{article['publication']}</td>\")\n",
    "    grand_html.append(f\"<td class=\\\"logit\\\">{article['logit']}</td>\")\n",
    "    grand_html.append(\"</tr>\")\n",
    "print(\"\\n\".join(grand_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
